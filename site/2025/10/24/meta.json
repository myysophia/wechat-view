{
  "aiInsights": {
    "overview": "2025年10月24日群内围绕LLM、token压缩、DeepSeek OCR及AI幻觉等技术话题展开热烈讨论，同时对行业浮躁现象有所反思。",
    "highlights": [
      "马工强调快速验证优于冗长讨论，体现敏捷开发思维",
      "DeepSeek OCR引发关于信息密度与多模态效率的深度探讨",
      "多位成员指出AI领域存在标题党与流量驱动的浮躁问题",
      "Michael等人讨论人才流动与技术体系可持续性"
    ],
    "opportunities": [
      "探索视觉token在长上下文压缩中的实际应用",
      "建立更健康的AI技术社区讨论氛围",
      "推动OCR等垂直场景的严谨技术验证"
    ],
    "risks": [
      "过度简化开发流程可能忽视系统稳定性",
      "技术讨论易被情绪化言论带偏（如“dba淘汰论”）",
      "未回应的业务咨询（如智谱销售）影响协作效率"
    ],
    "actions": [
      "跟进邹轶关于智谱销售的咨询",
      "整理DeepSeek OCR测试结果供群内参考",
      "引导对“空白输入幻觉”问题的系统性解决方案"
    ],
    "spotlight": "“写出来让AI干就完了，现在干根本不是瓶颈，瓶颈在想和定义”——陈明"
  },
  "date": "2025-10-24",
  "keyword": "",
  "summary": {
    "totalMessages": 145,
    "uniqueSenders": 28,
    "topSenders": [
      {
        "key": "Nick@保利威视频",
        "count": 19
      },
      {
        "key": "马工",
        "count": 19
      },
      {
        "key": "Player¹",
        "count": 18
      },
      {
        "key": "吴昊 Cubic",
        "count": 12
      },
      {
        "key": "Michael",
        "count": 11
      }
    ],
    "topLinks": [
      "https://mp.weixin.qq.com/s?__biz=MzkzNjUwODAwNw==\u0026mid=2247487618\u0026idx=1\u0026sn=6111129aa497de81cdd1dc960b2ade4c\u0026chksm=c3994a438d7ba19afb2bcd991dd20822d0f9b5d61939de2e7b06e10ce1c8f243e8399ff46d6f\u0026mpshare=1\u0026scene=1\u0026srcid=1024ma0PS80GTKM6HRkQRS1L\u0026sharer_shareinfo=324f7bb7ee07a2563d381d4f0ae5e29e\u0026sharer_shareinfo_first=720a0770c136fff727517f8d4882b53e#rd",
      "https://mp.weixin.qq.com/s/X5mkqB8x7CqNqmO-G5G3Fw",
      "https://nof1.ai/",
      "https://mp.weixin.qq.com/s?__biz=MzA5NzU3MDczNA==\u0026mid=2247489289\u0026idx=1\u0026sn=919a537ad25e536bfd179d5a4144031d\u0026chksm=9112f86f41a9dcd544741cfb29af58811fd336c7b3ea8a414262a8969abc32539b03f6295acc\u0026mpshare=1\u0026scene=1\u0026srcid=1024ymuWFTz0uRvQZaJKBdlY\u0026sharer_shareinfo=9771bf115a53face98d1169915014b2c\u0026sharer_shareinfo_first=9771bf115a53face98d1169915014b2c#rd",
      "https://mp.weixin.qq.com/s?__biz=MzIzOTU0NTQ0MA==\u0026mid=2247554461\u0026idx=1\u0026sn=e52d01dd0832ede6a6cc146c152b39a1\u0026chksm=e8b35b5af8820fa373291ccb76d41f7176c52bedf3c0ff596187ede95bf92a5ba73ef2064e17\u0026mpshare=1\u0026scene=1\u0026srcid=1024jZdvIUwqixmWrGStZR8E\u0026sharer_shareinfo=8e7eebafc39ba27d77990e4c87f0edac\u0026sharer_shareinfo_first=8e7eebafc39ba27d77990e4c87f0edac#rd"
    ],
    "hourlyHistogram": [
      11,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      1,
      0,
      0,
      31,
      44,
      5,
      53,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0
    ],
    "keywords": [
      {
        "key": "llm",
        "count": 28
      },
      {
        "key": "token",
        "count": 24
      },
      {
        "key": "信息",
        "count": 17
      },
      {
        "key": "文本",
        "count": 15
      },
      {
        "key": "幻觉",
        "count": 14
      },
      {
        "key": "模型",
        "count": 14
      },
      {
        "key": "非常",
        "count": 14
      },
      {
        "key": "压缩",
        "count": 13
      },
      {
        "key": "文字",
        "count": 13
      },
      {
        "key": "ocr",
        "count": 11
      },
      {
        "key": "了一",
        "count": 11
      },
      {
        "key": "出来",
        "count": 11
      },
      {
        "key": "的文",
        "count": 11
      },
      {
        "key": "deepseek",
        "count": 10
      },
      {
        "key": "图片",
        "count": 10
      },
      {
        "key": "问题",
        "count": 10
      },
      {
        "key": "tokenizer",
        "count": 9
      },
      {
        "key": "图像",
        "count": 9
      },
      {
        "key": "比如",
        "count": 9
      },
      {
        "key": "记忆",
        "count": 9
      }
    ],
    "peakHour": 14,
    "highlights": [
      "消息 145 条，活跃 28 人；峰值 14:00-14:59",
      "Top 发送者：Nick@保利威视频(19)、马工(19)、Player¹(18)",
      "热门主题：llm、token、信息",
      "热门链接 5 个，例如 mp.weixin.qq.com",
      "图片 17 张"
    ],
    "topics": [
      {
        "name": "llm",
        "keywords": [
          "llm"
        ],
        "count": 4,
        "representative": "是的。文字是符号，只需要用高效编码表达符号就可以了。llm会有更多的东西，比如 语义，embeding就是把文字转为语义吧。语义的维度越高，产生的token越大。"
      },
      {
        "name": "token",
        "keywords": [
          "token"
        ],
        "count": 11,
        "representative": "分享一些针对Deepseek OCR的学习笔记。注意这些材料在直观性和严谨性上更强调直观性。如果希望有严谨性的话还是得去看原论文。\n\nDeepSeekOCR想解决的问题其实跟 OCR 没关系。它的直接出发点非常 DeepSeek。就是 LLM 是基于 Transformer 的，Transformer context window 一长，它不论是占用的显存还是需要的算力都是平方量级增长的，又慢又贵。Deepseek用了一个非常直观的方法去部分解决这个问题：token 压缩。比如 100 个 token 的文本，我用其他模态比如图片，可能 10 个 token 就能表达出来同样的含义，这样就可以省很多钱，也可以用更小的显卡。\n\n但是如果真的要做这件事情，一个正统的方法是，我们把普通 LLM 前面的 tokenizer 从文本 tokenizer 换成图像的 tokenizer，然后从头训练一个 LLM，然后在各种任务上面测试，比如让它写程序、做问答、调用工具，看看效果会不会变好。很显然这个非常重量级。那有没有一个任务，它又能验证这种信息压缩的思路对不对，又能不要这么重量级，又足够有说服力呢？OCR 就是一个这样的例子。\n\n具体地说，DeepSeek 先是把文本渲染成图像，这样就知道原始信息ground truth是什么。然后再把图像用他们的 tokenizer 进行处理，得到一堆token，这一步跟文本得到token概念上类似。然后再用一个非常小的 3B 的 LLM 进行解码。就是问它：你从这些 token 里面能不能反推出来它最初输入的那些文本是什么。这就形成了一个定量的 evaluation 的框架。基于这个框架，他们给出了一个非常惊人的结果，原来有 1000 个 token 的文本，你把它渲染出来，通过控制 tokenizer 的参数和控制分辨率，让它只输出 100 个视觉 token，但就把这 100 个视觉 token 扔给这个 3B 的 LLM。它能够以 97% 的精度还原这 1000 个 token 的文本，这就是所谓的信息压缩率达到 10 倍。\n\n他们做的这段事情和这个结论有很大的创新意义。\n\n1. 目前主流多模态LLM都是纯文本的地位比图像持平或者更高。但DeepseekOCR提供了一些强有力的证据，来证明Vision作为一种modality，比text要更高效，信息密度更大。这跟以前大家的直觉完全不一样。与此同时很多视觉信息没办法用文本表示出来，但文本的信息可以直接渲染成图片。这就引导着多模态未来的发展方向可能会让视觉的地位要高于文本。这不是定论，但是个很大的变革。\n2. 它给LLM解码的过程也带来了很大的灵活性。LLM在解码的时候可以先用一个zoom out view来得到（literally）big picture。等它需要具体看某一个地方的时候它再把这块图像给放大截取出来做进一步分析。像这种流程，如果在文本模态上做这一步会非常artificial。但是在图像模态上做这一步非常自然：就是图像的放大和缩小。\n3. 这个和记忆也有关。人类的记忆是分层的，短期记忆往往是非常精确的，长期记忆就只记得一个大概。这个天然就对应着上面说的放缩解码过程。长期记忆就缩小一点，短期记忆就放大一点。所以DeepseekOCR提供了一种非常自然的多层次记忆/context windows compression的实现方法，甚至可以理论上可以做一种无限延伸的记忆。\n\n当然，这只是一个早期的工作，比如OCR做得好并不等于推理、幻觉等等方面也会很好。最终视觉模态会不会比文本更好需要更多的实验。我们讨论的多层次记忆很让人激动，但目前DeepseekOCR只实现了（非常精巧的）tokenizer内部的多分辨率编码。LLM交互式地主动去凑近了看，这个还是我们的构想，不是说已经有开源实现了。但总的来说，这个思路和初步的实验结果给了人很多想象空间。"
      },
      {
        "name": "信息",
        "keywords": [
          "信息"
        ],
        "count": 9,
        "representative": "分享一些针对Deepseek OCR的学习笔记。注意这些材料在直观性和严谨性上更强调直观性。如果希望有严谨性的话还是得去看原论文。\n\nDeepSeekOCR想解决的问题其实跟 OCR 没关系。它的直接出发点非常 DeepSeek。就是 LLM 是基于 Transformer 的，Transformer context window 一长，它不论是占用的显存还是需要的算力都是平方量级增长的，又慢又贵。Deepseek用了一个非常直观的方法去部分解决这个问题：token 压缩。比如 100 个 token 的文本，我用其他模态比如图片，可能 10 个 token 就能表达出来同样的含义，这样就可以省很多钱，也可以用更小的显卡。\n\n但是如果真的要做这件事情，一个正统的方法是，我们把普通 LLM 前面的 tokenizer 从文本 tokenizer 换成图像的 tokenizer，然后从头训练一个 LLM，然后在各种任务上面测试，比如让它写程序、做问答、调用工具，看看效果会不会变好。很显然这个非常重量级。那有没有一个任务，它又能验证这种信息压缩的思路对不对，又能不要这么重量级，又足够有说服力呢？OCR 就是一个这样的例子。\n\n具体地说，DeepSeek 先是把文本渲染成图像，这样就知道原始信息ground truth是什么。然后再把图像用他们的 tokenizer 进行处理，得到一堆token，这一步跟文本得到token概念上类似。然后再用一个非常小的 3B 的 LLM 进行解码。就是问它：你从这些 token 里面能不能反推出来它最初输入的那些文本是什么。这就形成了一个定量的 evaluation 的框架。基于这个框架，他们给出了一个非常惊人的结果，原来有 1000 个 token 的文本，你把它渲染出来，通过控制 tokenizer 的参数和控制分辨率，让它只输出 100 个视觉 token，但就把这 100 个视觉 token 扔给这个 3B 的 LLM。它能够以 97% 的精度还原这 1000 个 token 的文本，这就是所谓的信息压缩率达到 10 倍。\n\n他们做的这段事情和这个结论有很大的创新意义。\n\n1. 目前主流多模态LLM都是纯文本的地位比图像持平或者更高。但DeepseekOCR提供了一些强有力的证据，来证明Vision作为一种modality，比text要更高效，信息密度更大。这跟以前大家的直觉完全不一样。与此同时很多视觉信息没办法用文本表示出来，但文本的信息可以直接渲染成图片。这就引导着多模态未来的发展方向可能会让视觉的地位要高于文本。这不是定论，但是个很大的变革。\n2. 它给LLM解码的过程也带来了很大的灵活性。LLM在解码的时候可以先用一个zoom out view来得到（literally）big picture。等它需要具体看某一个地方的时候它再把这块图像给放大截取出来做进一步分析。像这种流程，如果在文本模态上做这一步会非常artificial。但是在图像模态上做这一步非常自然：就是图像的放大和缩小。\n3. 这个和记忆也有关。人类的记忆是分层的，短期记忆往往是非常精确的，长期记忆就只记得一个大概。这个天然就对应着上面说的放缩解码过程。长期记忆就缩小一点，短期记忆就放大一点。所以DeepseekOCR提供了一种非常自然的多层次记忆/context windows compression的实现方法，甚至可以理论上可以做一种无限延伸的记忆。\n\n当然，这只是一个早期的工作，比如OCR做得好并不等于推理、幻觉等等方面也会很好。最终视觉模态会不会比文本更好需要更多的实验。我们讨论的多层次记忆很让人激动，但目前DeepseekOCR只实现了（非常精巧的）tokenizer内部的多分辨率编码。LLM交互式地主动去凑近了看，这个还是我们的构想，不是说已经有开源实现了。但总的来说，这个思路和初步的实验结果给了人很多想象空间。"
      },
      {
        "name": "文本",
        "keywords": [
          "文本"
        ],
        "count": 3,
        "representative": "分享一些针对Deepseek OCR的学习笔记。注意这些材料在直观性和严谨性上更强调直观性。如果希望有严谨性的话还是得去看原论文。\n\nDeepSeekOCR想解决的问题其实跟 OCR 没关系。它的直接出发点非常 DeepSeek。就是 LLM 是基于 Transformer 的，Transformer context window 一长，它不论是占用的显存还是需要的算力都是平方量级增长的，又慢又贵。Deepseek用了一个非常直观的方法去部分解决这个问题：token 压缩。比如 100 个 token 的文本，我用其他模态比如图片，可能 10 个 token 就能表达出来同样的含义，这样就可以省很多钱，也可以用更小的显卡。\n\n但是如果真的要做这件事情，一个正统的方法是，我们把普通 LLM 前面的 tokenizer 从文本 tokenizer 换成图像的 tokenizer，然后从头训练一个 LLM，然后在各种任务上面测试，比如让它写程序、做问答、调用工具，看看效果会不会变好。很显然这个非常重量级。那有没有一个任务，它又能验证这种信息压缩的思路对不对，又能不要这么重量级，又足够有说服力呢？OCR 就是一个这样的例子。\n\n具体地说，DeepSeek 先是把文本渲染成图像，这样就知道原始信息ground truth是什么。然后再把图像用他们的 tokenizer 进行处理，得到一堆token，这一步跟文本得到token概念上类似。然后再用一个非常小的 3B 的 LLM 进行解码。就是问它：你从这些 token 里面能不能反推出来它最初输入的那些文本是什么。这就形成了一个定量的 evaluation 的框架。基于这个框架，他们给出了一个非常惊人的结果，原来有 1000 个 token 的文本，你把它渲染出来，通过控制 tokenizer 的参数和控制分辨率，让它只输出 100 个视觉 token，但就把这 100 个视觉 token 扔给这个 3B 的 LLM。它能够以 97% 的精度还原这 1000 个 token 的文本，这就是所谓的信息压缩率达到 10 倍。\n\n他们做的这段事情和这个结论有很大的创新意义。\n\n1. 目前主流多模态LLM都是纯文本的地位比图像持平或者更高。但DeepseekOCR提供了一些强有力的证据，来证明Vision作为一种modality，比text要更高效，信息密度更大。这跟以前大家的直觉完全不一样。与此同时很多视觉信息没办法用文本表示出来，但文本的信息可以直接渲染成图片。这就引导着多模态未来的发展方向可能会让视觉的地位要高于文本。这不是定论，但是个很大的变革。\n2. 它给LLM解码的过程也带来了很大的灵活性。LLM在解码的时候可以先用一个zoom out view来得到（literally）big picture。等它需要具体看某一个地方的时候它再把这块图像给放大截取出来做进一步分析。像这种流程，如果在文本模态上做这一步会非常artificial。但是在图像模态上做这一步非常自然：就是图像的放大和缩小。\n3. 这个和记忆也有关。人类的记忆是分层的，短期记忆往往是非常精确的，长期记忆就只记得一个大概。这个天然就对应着上面说的放缩解码过程。长期记忆就缩小一点，短期记忆就放大一点。所以DeepseekOCR提供了一种非常自然的多层次记忆/context windows compression的实现方法，甚至可以理论上可以做一种无限延伸的记忆。\n\n当然，这只是一个早期的工作，比如OCR做得好并不等于推理、幻觉等等方面也会很好。最终视觉模态会不会比文本更好需要更多的实验。我们讨论的多层次记忆很让人激动，但目前DeepseekOCR只实现了（非常精巧的）tokenizer内部的多分辨率编码。LLM交互式地主动去凑近了看，这个还是我们的构想，不是说已经有开源实现了。但总的来说，这个思路和初步的实验结果给了人很多想象空间。"
      },
      {
        "name": "模型",
        "keywords": [
          "模型"
        ],
        "count": 9,
        "representative": "# LLM 在空白输入时的幻觉问题\n\n## 背景\n\n在日常使用 LLM 的过程中，我和团队成员都注意到一个共性问题：当输入内容为空白或信息量极少时，LLM 的幻觉现象会格外明显。本文整理了几个实际场景中遇到的典型案例，以及对应的应对思路。\n\n## 几个典型场景\n\n### 代码审查中的\"假 bug\"\n\n同事在使用 LLM 辅助 Coding 时发现，即使代码没有明显 bug，LLM 也会幻觉出一些问题并给出修复建议。这种\"无中生有\"的现象在输入信息不足时尤为突出。\n\n### 空白文档的\"续写\"陷阱\n\n我的使用经验是：当提供的文档为空白时，LLM 硬是生成一些像模像样的内容。反而是给他内容太多了，他会遗漏不少内容。因此幻觉往往发生在无中生有时，此时更像是在触发大模型的续写特性。\n\n### 视频翻译中的静音幻觉\n\n最近做长视频翻译时，当音频静音时，whisper 这类基于 LLM 技术的模型会在空白时段生成一些莫名其妙的文字，往往是常见的\"谢谢\"\"就这样\"之类的。\n\n因为没有对应的音色，后续的 TTS 引擎也会用默认的声音输出，特别滑稽：B 站的 indextts 会用动漫妹子的夹子音在严肃的会议视频里插上两句。\n\n#### 解决方案：VAD 与 whisper 参数\n\n朋友们建议使用 VAD 去掉无人声部分。\n\n**VAD** 是 **Voice Activity Detection（语音活动检测）** 的缩写，是一种用来判断语音信号中是否存在人声的技术。它是语音处理、语音识别、语音通信等系统中的一个基础模块。\n\nwhisper 的参数中也有对应的参数和使用建议，避免这种幻觉。我通过问 ChatGPT 找到了参数，并解决了一些问题，但还是会有一些幻觉，VAD 可能是有必要的。\n\n### OCR 场景的类似问题\n\n以此类推，那些 LLM 生成的 OCR 模型方案，估计在处理空白的图片时，也会生成幻觉 OCR，会识别出莫名其妙的常见语。\n\n## 技术原理与应对\n\n这些问题是 LLM 从训练实现中的技术原理带来的。我们熟知以后，就可以避免。\n\n同样，当你给他的文章没有知识可洞察时，让他去列出知识，他就会生成一些幻觉知识和见解，有可能误导你。\n\n当然，读者对原始内容是否有洞见有观点的判断力都没有时，LLM 幻觉生成的是有可能让他相信的。当上当受骗以后，就会赖 LLM 幻觉骗了他。问题在谁？\n\n## 小结\n\n空白输入是 LLM 幻觉的高发场景。无论是代码审查、文档生成、语音识别还是图像 OCR，当输入信息不足时，模型会倾向于\"续写\"常见内容。了解这一特性后，我们可以通过预处理（如 VAD）、调整模型参数或提高输入质量来减少幻觉的影响。同时，保持对输出结果的批判性审视也是必要的。\n"
      }
    ],
    "imageCount": 17,
    "groupVibes": {
      "score": 59,
      "activity": 1,
      "sentiment": 0.5,
      "infoDensity": 0.32,
      "controversy": 0.06,
      "tone": "讨论平稳",
      "reasons": [
        "活跃度高（145 条、28 人参与）",
        "讨论较温和，可适度引导观点碰撞"
      ]
    },
    "replyDebt": {
      "outstanding": [
        {
          "questioner": "邹轶",
          "question": "请问下，智谱这块大家有熟悉的销售嘛",
          "askedAt": "2025-10-24T11:16:28+08:00",
          "ageMinutes": 223
        },
        {
          "questioner": "小米-AI全栈开发寻AI创业合作",
          "question": "测试的效果怎么样呢？",
          "askedAt": "2025-10-24T12:16:37+08:00",
          "ageMinutes": 162.9
        },
        {
          "questioner": "李峻",
          "question": "# LLM 在空白输入时的幻觉问题\n\n## 背景\n\n在日常使用 LLM 的过程中，我和团队成员都注意到一个共性问题：当输入内容为空白或信息量极少时，LLM 的幻觉现象会格外明显。本文整理了几个实际场景中遇到的典型案例，以及对应的应对思路。\n…",
          "askedAt": "2025-10-24T12:27:00+08:00",
          "ageMinutes": 152.5
        },
        {
          "questioner": "薇冷 Violet",
          "question": "编码和语义是不是两个概念呢？",
          "askedAt": "2025-10-24T13:42:19+08:00",
          "ageMinutes": 77.2
        }
      ],
      "resolved": [
        {
          "questioner": "鸭哥",
          "question": "分享一些针对Deepseek OCR的学习笔记。注意这些材料在直观性和严谨性上更强调直观性。如果希望有严谨性的话还是得去看原论文。\n\nDeepSeekOCR想解决的问题其实跟 OCR 没关系。它的直接出发点非常 DeepSeek。就是 LL…",
          "askedAt": "2025-10-24T11:55:14+08:00",
          "responseMinutes": 8.8,
          "responders": [
            "吴昊 Cubic"
          ]
        },
        {
          "questioner": "吴昊 Cubic",
          "question": "号称了啥，反正论文和开源模型都在。容易验证也容易证伪的。但是我觉得这个方向的确是在触及信息论的底层基础。庞大的信息量是如何被我们可怜的脑计算机处理的。特斯拉不走寻常路的智驾方案建立在什么理论上。都是这个方面的研究",
          "askedAt": "2025-10-24T12:11:05+08:00",
          "responseMinutes": 5.5,
          "responders": [
            "小米-AI全栈开发寻AI创业合作"
          ]
        },
        {
          "questioner": "详志(ip)",
          "question": "你这时间差，能赚到钱？",
          "askedAt": "2025-10-24T14:35:06+08:00",
          "responseMinutes": 5.7,
          "responders": [
            "Nick@保利威视频"
          ]
        }
      ],
      "avgResponseMinutes": 6.7,
      "bestResponseHours": [
        12,
        14
      ]
    }
  },
  "talker": "27587714869@chatroom"
}
