{
  "aiInsights": {
    "overview": "2025年10月24日群内围绕AI开发效率、DeepSeek OCR技术原理及信息论争议展开热烈讨论，同时对媒体标题党现象表达普遍不满。",
    "highlights": [
      "马工强调快速验证优于冗长讨论，体现敏捷开发思维",
      "DeepSeek OCR的token压缩机制引发信息论层面技术辩论",
      "多位成员批评AI相关自媒体为流量硬蹭热点、内容失真",
      "群内对LLM幻觉、上下文管理、MCP架构等话题有深度探讨"
    ],
    "opportunities": [
      "可组织DeepSeek OCR实测分享会，验证10倍压缩效果",
      "梳理MCP与系统层级类比，形成开发者共识模型",
      "建立高质量AI技术内容推荐机制，对抗标题党"
    ],
    "risks": [
      "技术讨论易陷入术语之争，如token与信息论基础理解分歧",
      "对传统岗位（如DBA）的贬低言论可能引发群体对立",
      "部分成员对“三天开发APP”等说法持怀疑态度，存在信任裂痕"
    ],
    "actions": [
      "跟进邹轶关于智谱销售的求助，对接相关资源",
      "整理DeepSeek OCR讨论要点，形成技术简报",
      "引导关于LLM幻觉问题的解决方案沉淀为最佳实践"
    ],
    "spotlight": "“写出来让AI干就完了，现在干根本不是瓶颈，瓶颈在想和定义”——陈明"
  },
  "date": "2025-10-24",
  "keyword": "",
  "summary": {
    "totalMessages": 426,
    "uniqueSenders": 38,
    "topSenders": [
      {
        "key": "Michael",
        "count": 61
      },
      {
        "key": "Nick@保利威视频",
        "count": 53
      },
      {
        "key": "Player¹",
        "count": 47
      },
      {
        "key": "马工",
        "count": 27
      },
      {
        "key": "薇冷 Violet",
        "count": 25
      }
    ],
    "topLinks": [
      "https://mp.weixin.qq.com/s?__biz=MzkzNzU0OTg0Mw==\u0026mid=2247484660\u0026idx=1\u0026sn=0887bf96465b3ff564e47bbe4ef41ea6\u0026chksm=c3ce46b63d33dad9240dbfd3a10fbd2c6e4e8565edae0c9ca6ab53742faae4f2c44a5d964450\u0026mpshare=1\u0026scene=1\u0026srcid=1022K9zruSJoG3tTLKwxKBhy\u0026sharer_shareinfo=ab55a63dee8efbc7ce4d23cc95ee9317\u0026sharer_shareinfo_first=ab55a63dee8efbc7ce4d23cc95ee9317#rd",
      "https://www.xiaohongshu.com/discovery/item/68fb0fec00000000040041de?app_platform=ios\u0026app_version=9.4\u0026share_from_user_hidden=true\u0026xsec_source=app_share\u0026type=normal\u0026xsec_token=CBxoqinwZouAgsqytuuveKo9kfFLF8-BVb6qR9cwuKd2w=\u0026author_share=1\u0026xhsshare=WeixinSession\u0026shareRedId=N0g4MDw3NU5LP0ZFO0o1OzY0OUs7PjlL\u0026apptime=1761302543\u0026share_id=0755b58557794657b9903f41fe87c545",
      "https://mp.weixin.qq.com/s?__biz=MzIzOTU0NTQ0MA==\u0026mid=2247554461\u0026idx=1\u0026sn=e52d01dd0832ede6a6cc146c152b39a1\u0026chksm=e8b35b5af8820fa373291ccb76d41f7176c52bedf3c0ff596187ede95bf92a5ba73ef2064e17\u0026mpshare=1\u0026scene=1\u0026srcid=1024jZdvIUwqixmWrGStZR8E\u0026sharer_shareinfo=8e7eebafc39ba27d77990e4c87f0edac\u0026sharer_shareinfo_first=8e7eebafc39ba27d77990e4c87f0edac#rd",
      "https://mp.weixin.qq.com/s?__biz=MzkzNjUwODAwNw==\u0026mid=2247487618\u0026idx=1\u0026sn=6111129aa497de81cdd1dc960b2ade4c\u0026chksm=c3994a438d7ba19afb2bcd991dd20822d0f9b5d61939de2e7b06e10ce1c8f243e8399ff46d6f\u0026mpshare=1\u0026scene=1\u0026srcid=1024ma0PS80GTKM6HRkQRS1L\u0026sharer_shareinfo=324f7bb7ee07a2563d381d4f0ae5e29e\u0026sharer_shareinfo_first=720a0770c136fff727517f8d4882b53e#rd",
      "https://mp.weixin.qq.com/s?__biz=MzA5NzU3MDczNA==\u0026mid=2247489307\u0026idx=1\u0026sn=faf778e79568d8bc20db06f6d574636f\u0026chksm=911bc12a669ab5932491ce830bfdf705d645eabcc115351c85d6a039e0bbb645aaeea02aaf88\u0026mpshare=1\u0026scene=1\u0026srcid=1024lTUrJQ5WZZu5V28ThN7f\u0026sharer_shareinfo=96ba50730695c210a608855623fc580b\u0026sharer_shareinfo_first=96ba50730695c210a608855623fc580b#rd"
    ],
    "hourlyHistogram": [
      11,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      1,
      0,
      0,
      31,
      44,
      5,
      53,
      98,
      15,
      82,
      50,
      28,
      5,
      3,
      0,
      0
    ],
    "keywords": [
      {
        "key": "llm",
        "count": 39
      },
      {
        "key": "mcp",
        "count": 36
      },
      {
        "key": "模型",
        "count": 31
      },
      {
        "key": "token",
        "count": 25
      },
      {
        "key": "现在",
        "count": 22
      },
      {
        "key": "信息",
        "count": 21
      },
      {
        "key": "是一",
        "count": 21
      },
      {
        "key": "数据",
        "count": 20
      },
      {
        "key": "系统",
        "count": 20
      },
      {
        "key": "问题",
        "count": 20
      },
      {
        "key": "上下",
        "count": 18
      },
      {
        "key": "上下文",
        "count": 18
      },
      {
        "key": "下文",
        "count": 18
      },
      {
        "key": "理解",
        "count": 17
      },
      {
        "key": "了一",
        "count": 16
      },
      {
        "key": "比如",
        "count": 16
      },
      {
        "key": "用户",
        "count": 16
      },
      {
        "key": "非常",
        "count": 16
      },
      {
        "key": "skill",
        "count": 15
      },
      {
        "key": "很多",
        "count": 15
      }
    ],
    "peakHour": 15,
    "highlights": [
      "消息 426 条，活跃 38 人；峰值 15:00-15:59",
      "Top 发送者：Michael(61)、Nick@保利威视频(53)、Player¹(47)",
      "热门主题：llm、mcp、模型",
      "热门链接 5 个，例如 mp.weixin.qq.com",
      "图片 27 张"
    ],
    "topics": [
      {
        "name": "llm",
        "keywords": [
          "llm"
        ],
        "count": 5,
        "representative": "是的。文字是符号，只需要用高效编码表达符号就可以了。llm会有更多的东西，比如 语义，embeding就是把文字转为语义吧。语义的维度越高，产生的token越大。"
      },
      {
        "name": "mcp",
        "keywords": [
          "mcp"
        ],
        "count": 25,
        "representative": "1，寄存器 （sys_promt）\n2，一级缓存(user_promt,user query)\n3，二级缓存(session history)\n4，内存(chat history)\n5，SSD（文件，数据库等）(mcp,skill,subagent)"
      },
      {
        "name": "模型",
        "keywords": [
          "模型"
        ],
        "count": 23,
        "representative": "# LLM 在空白输入时的幻觉问题\n\n## 背景\n\n在日常使用 LLM 的过程中，我和团队成员都注意到一个共性问题：当输入内容为空白或信息量极少时，LLM 的幻觉现象会格外明显。本文整理了几个实际场景中遇到的典型案例，以及对应的应对思路。\n\n## 几个典型场景\n\n### 代码审查中的\"假 bug\"\n\n同事在使用 LLM 辅助 Coding 时发现，即使代码没有明显 bug，LLM 也会幻觉出一些问题并给出修复建议。这种\"无中生有\"的现象在输入信息不足时尤为突出。\n\n### 空白文档的\"续写\"陷阱\n\n我的使用经验是：当提供的文档为空白时，LLM 硬是生成一些像模像样的内容。反而是给他内容太多了，他会遗漏不少内容。因此幻觉往往发生在无中生有时，此时更像是在触发大模型的续写特性。\n\n### 视频翻译中的静音幻觉\n\n最近做长视频翻译时，当音频静音时，whisper 这类基于 LLM 技术的模型会在空白时段生成一些莫名其妙的文字，往往是常见的\"谢谢\"\"就这样\"之类的。\n\n因为没有对应的音色，后续的 TTS 引擎也会用默认的声音输出，特别滑稽：B 站的 indextts 会用动漫妹子的夹子音在严肃的会议视频里插上两句。\n\n#### 解决方案：VAD 与 whisper 参数\n\n朋友们建议使用 VAD 去掉无人声部分。\n\n**VAD** 是 **Voice Activity Detection（语音活动检测）** 的缩写，是一种用来判断语音信号中是否存在人声的技术。它是语音处理、语音识别、语音通信等系统中的一个基础模块。\n\nwhisper 的参数中也有对应的参数和使用建议，避免这种幻觉。我通过问 ChatGPT 找到了参数，并解决了一些问题，但还是会有一些幻觉，VAD 可能是有必要的。\n\n### OCR 场景的类似问题\n\n以此类推，那些 LLM 生成的 OCR 模型方案，估计在处理空白的图片时，也会生成幻觉 OCR，会识别出莫名其妙的常见语。\n\n## 技术原理与应对\n\n这些问题是 LLM 从训练实现中的技术原理带来的。我们熟知以后，就可以避免。\n\n同样，当你给他的文章没有知识可洞察时，让他去列出知识，他就会生成一些幻觉知识和见解，有可能误导你。\n\n当然，读者对原始内容是否有洞见有观点的判断力都没有时，LLM 幻觉生成的是有可能让他相信的。当上当受骗以后，就会赖 LLM 幻觉骗了他。问题在谁？\n\n## 小结\n\n空白输入是 LLM 幻觉的高发场景。无论是代码审查、文档生成、语音识别还是图像 OCR，当输入信息不足时，模型会倾向于\"续写\"常见内容。了解这一特性后，我们可以通过预处理（如 VAD）、调整模型参数或提高输入质量来减少幻觉的影响。同时，保持对输出结果的批判性审视也是必要的。\n"
      },
      {
        "name": "token",
        "keywords": [
          "token"
        ],
        "count": 12,
        "representative": "分享一些针对Deepseek OCR的学习笔记。注意这些材料在直观性和严谨性上更强调直观性。如果希望有严谨性的话还是得去看原论文。\n\nDeepSeekOCR想解决的问题其实跟 OCR 没关系。它的直接出发点非常 DeepSeek。就是 LLM 是基于 Transformer 的，Transformer context window 一长，它不论是占用的显存还是需要的算力都是平方量级增长的，又慢又贵。Deepseek用了一个非常直观的方法去部分解决这个问题：token 压缩。比如 100 个 token 的文本，我用其他模态比如图片，可能 10 个 token 就能表达出来同样的含义，这样就可以省很多钱，也可以用更小的显卡。\n\n但是如果真的要做这件事情，一个正统的方法是，我们把普通 LLM 前面的 tokenizer 从文本 tokenizer 换成图像的 tokenizer，然后从头训练一个 LLM，然后在各种任务上面测试，比如让它写程序、做问答、调用工具，看看效果会不会变好。很显然这个非常重量级。那有没有一个任务，它又能验证这种信息压缩的思路对不对，又能不要这么重量级，又足够有说服力呢？OCR 就是一个这样的例子。\n\n具体地说，DeepSeek 先是把文本渲染成图像，这样就知道原始信息ground truth是什么。然后再把图像用他们的 tokenizer 进行处理，得到一堆token，这一步跟文本得到token概念上类似。然后再用一个非常小的 3B 的 LLM 进行解码。就是问它：你从这些 token 里面能不能反推出来它最初输入的那些文本是什么。这就形成了一个定量的 evaluation 的框架。基于这个框架，他们给出了一个非常惊人的结果，原来有 1000 个 token 的文本，你把它渲染出来，通过控制 tokenizer 的参数和控制分辨率，让它只输出 100 个视觉 token，但就把这 100 个视觉 token 扔给这个 3B 的 LLM。它能够以 97% 的精度还原这 1000 个 token 的文本，这就是所谓的信息压缩率达到 10 倍。\n\n他们做的这段事情和这个结论有很大的创新意义。\n\n1. 目前主流多模态LLM都是纯文本的地位比图像持平或者更高。但DeepseekOCR提供了一些强有力的证据，来证明Vision作为一种modality，比text要更高效，信息密度更大。这跟以前大家的直觉完全不一样。与此同时很多视觉信息没办法用文本表示出来，但文本的信息可以直接渲染成图片。这就引导着多模态未来的发展方向可能会让视觉的地位要高于文本。这不是定论，但是个很大的变革。\n2. 它给LLM解码的过程也带来了很大的灵活性。LLM在解码的时候可以先用一个zoom out view来得到（literally）big picture。等它需要具体看某一个地方的时候它再把这块图像给放大截取出来做进一步分析。像这种流程，如果在文本模态上做这一步会非常artificial。但是在图像模态上做这一步非常自然：就是图像的放大和缩小。\n3. 这个和记忆也有关。人类的记忆是分层的，短期记忆往往是非常精确的，长期记忆就只记得一个大概。这个天然就对应着上面说的放缩解码过程。长期记忆就缩小一点，短期记忆就放大一点。所以DeepseekOCR提供了一种非常自然的多层次记忆/context windows compression的实现方法，甚至可以理论上可以做一种无限延伸的记忆。\n\n当然，这只是一个早期的工作，比如OCR做得好并不等于推理、幻觉等等方面也会很好。最终视觉模态会不会比文本更好需要更多的实验。我们讨论的多层次记忆很让人激动，但目前DeepseekOCR只实现了（非常精巧的）tokenizer内部的多分辨率编码。LLM交互式地主动去凑近了看，这个还是我们的构想，不是说已经有开源实现了。但总的来说，这个思路和初步的实验结果给了人很多想象空间。"
      },
      {
        "name": "现在",
        "keywords": [
          "现在"
        ],
        "count": 22,
        "representative": "这种活以前光可行性分析就要讨论一个月。真要做的话，还得正儿八经的立项，成立（项目）团队，分工，各种准备做完都已经过去三个月了。\n\n现在我两个人白板上画一下，然后claude code商量下，觉得可行，找到cto立下军令状就开干了"
      }
    ],
    "imageCount": 27,
    "groupVibes": {
      "score": 60,
      "activity": 1,
      "sentiment": 0.49,
      "infoDensity": 0.3,
      "controversy": 0.1,
      "tone": "讨论平稳",
      "reasons": [
        "活跃度高（426 条、38 人参与）",
        "讨论较温和，可适度引导观点碰撞"
      ]
    },
    "replyDebt": {
      "outstanding": [
        {
          "questioner": "邹轶",
          "question": "请问下，智谱这块大家有熟悉的销售嘛",
          "askedAt": "2025-10-24T11:16:28+08:00",
          "ageMinutes": 639.5
        },
        {
          "questioner": "李峻",
          "question": "# LLM 在空白输入时的幻觉问题\n\n## 背景\n\n在日常使用 LLM 的过程中，我和团队成员都注意到一个共性问题：当输入内容为空白或信息量极少时，LLM 的幻觉现象会格外明显。本文整理了几个实际场景中遇到的典型案例，以及对应的应对思路。\n…",
          "askedAt": "2025-10-24T12:27:00+08:00",
          "ageMinutes": 569
        },
        {
          "questioner": "Nemo",
          "question": "mcp还会返回部分prompt插入到system prompt中？",
          "askedAt": "2025-10-24T15:46:05+08:00",
          "ageMinutes": 369.9
        },
        {
          "questioner": "Michael",
          "question": "能不能翻译成生活中的白话",
          "askedAt": "2025-10-24T18:27:10+08:00",
          "ageMinutes": 208.8
        },
        {
          "questioner": "Michael",
          "question": "现做OCR结构化？",
          "askedAt": "2025-10-24T19:29:43+08:00",
          "ageMinutes": 146.3
        }
      ],
      "resolved": [
        {
          "questioner": "鸭哥",
          "question": "分享一些针对Deepseek OCR的学习笔记。注意这些材料在直观性和严谨性上更强调直观性。如果希望有严谨性的话还是得去看原论文。\n\nDeepSeekOCR想解决的问题其实跟 OCR 没关系。它的直接出发点非常 DeepSeek。就是 LL…",
          "askedAt": "2025-10-24T11:55:14+08:00",
          "responseMinutes": 8.8,
          "responders": [
            "吴昊 Cubic"
          ]
        },
        {
          "questioner": "吴昊 Cubic",
          "question": "号称了啥，反正论文和开源模型都在。容易验证也容易证伪的。但是我觉得这个方向的确是在触及信息论的底层基础。庞大的信息量是如何被我们可怜的脑计算机处理的。特斯拉不走寻常路的智驾方案建立在什么理论上。都是这个方面的研究",
          "askedAt": "2025-10-24T12:11:05+08:00",
          "responseMinutes": 5.5,
          "responders": [
            "小米-AI全栈开发寻AI创业合作"
          ]
        },
        {
          "questioner": "小米-AI全栈开发寻AI创业合作",
          "question": "测试的效果怎么样呢？",
          "askedAt": "2025-10-24T12:16:37+08:00",
          "responseMinutes": 178.3,
          "responders": [
            "Nick@保利威视频"
          ]
        },
        {
          "questioner": "薇冷 Violet",
          "question": "编码和语义是不是两个概念呢？",
          "askedAt": "2025-10-24T13:42:19+08:00",
          "responseMinutes": 283,
          "responders": [
            "linhow"
          ]
        },
        {
          "questioner": "详志(ip)",
          "question": "你这时间差，能赚到钱？",
          "askedAt": "2025-10-24T14:35:06+08:00",
          "responseMinutes": 5.7,
          "responders": [
            "Nick@保利威视频"
          ]
        },
        {
          "questioner": "leo",
          "question": "怎么跟单别人的下单？",
          "askedAt": "2025-10-24T15:05:10+08:00",
          "responseMinutes": 9.4,
          "responders": [
            "Nick@保利威视频"
          ]
        },
        {
          "questioner": "leo",
          "question": "AI还有持仓的吗？",
          "askedAt": "2025-10-24T15:13:44+08:00",
          "responseMinutes": 0.8,
          "responders": [
            "Nick@保利威视频"
          ]
        },
        {
          "questioner": "Young",
          "question": "是这个网站页面上爬的持仓数据吗？",
          "askedAt": "2025-10-24T15:14:23+08:00",
          "responseMinutes": 0.3,
          "responders": [
            "Nick@保利威视频"
          ]
        },
        {
          "questioner": "Nick@保利威视频",
          "question": "还好, 一些固定流程就很好. 比如: playwright的mcp就会返回一个如何生成测试代码的prompt指导ai在执行完所有步骤之后生成测试代码沉淀下来",
          "askedAt": "2025-10-24T15:47:34+08:00",
          "responseMinutes": 5.2,
          "responders": [
            "Michael"
          ]
        },
        {
          "questioner": "丁三金",
          "question": "8n8,n8n？用来做流程创建？",
          "askedAt": "2025-10-24T16:07:03+08:00",
          "responseMinutes": 0.6,
          "responders": [
            "Michael"
          ]
        },
        {
          "questioner": "陈明",
          "question": "n8n 执行速度怎么样，只有调用 LLM 的慢是吗",
          "askedAt": "2025-10-24T16:12:00+08:00",
          "responseMinutes": 0.9,
          "responders": [
            "Michael"
          ]
        },
        {
          "questioner": "韩亮",
          "question": "大家怎么看dify之类的低代码平台？ 真的会有很多的应用开发，基于这些低代码平台吗？",
          "askedAt": "2025-10-24T17:11:01+08:00",
          "responseMinutes": 0.8,
          "responders": [
            "Lex"
          ]
        },
        {
          "questioner": "韩亮",
          "question": "所以，低代码平台的主要use case就是demo？",
          "askedAt": "2025-10-24T17:12:11+08:00",
          "responseMinutes": 3.2,
          "responders": [
            "丁三金"
          ]
        },
        {
          "questioner": "马工",
          "question": "mcp占用上下文的问题，为什么不能通过sub agent解决？",
          "askedAt": "2025-10-24T17:30:26+08:00",
          "responseMinutes": 4.3,
          "responders": [
            "Nick@保利威视频"
          ]
        },
        {
          "questioner": "linhow",
          "question": "映射到agent上下文，下面这些概念应该是什么：\n1，寄存器\n2，一级缓存\n3，二级缓存\n4，内存\n5，SSD（文件，数据库等）\n\n或者说，上面这样类比是不对的？",
          "askedAt": "2025-10-24T17:52:19+08:00",
          "responseMinutes": 30.7,
          "responders": [
            "薇冷 Violet"
          ]
        },
        {
          "questioner": "linhow",
          "question": "我们现在讲的context（sys+user），是不是都属于寄存器这一层？",
          "askedAt": "2025-10-24T17:53:11+08:00",
          "responseMinutes": 29.8,
          "responders": [
            "薇冷 Violet"
          ]
        },
        {
          "questioner": "吴昊 Cubic",
          "question": "请教各位ai上仙。现在大家做chatbot的时候。如何让ai更好的理解流程图和图片信息的[呲牙][抱拳][抱拳]",
          "askedAt": "2025-10-24T17:55:37+08:00",
          "responseMinutes": 2.4,
          "responders": [
            "马工"
          ]
        },
        {
          "questioner": "薇冷 Violet",
          "question": "为啥还要翻译成生活中的白话？我觉得这已经是开发者很容易接受的一个类比了",
          "askedAt": "2025-10-24T18:28:21+08:00",
          "responseMinutes": 13.3,
          "responders": [
            "linhow"
          ]
        },
        {
          "questioner": "薇冷 Violet",
          "question": "终端用户管你三七二十一的，为什么要让终端用户理解？",
          "askedAt": "2025-10-24T18:28:48+08:00",
          "responseMinutes": 12.9,
          "responders": [
            "linhow"
          ]
        },
        {
          "questioner": "薇冷 Violet",
          "question": "你何必向他们试图解释没必要的东西呢？",
          "askedAt": "2025-10-24T18:31:20+08:00",
          "responseMinutes": 10.3,
          "responders": [
            "linhow"
          ]
        },
        {
          "questioner": "JY.王津银@ElevoAi",
          "question": "MCP理解成系统调用syscall是不是更好点？",
          "askedAt": "2025-10-24T18:50:05+08:00",
          "responseMinutes": 1.8,
          "responders": [
            "linhow"
          ]
        },
        {
          "questioner": "linhow",
          "question": "无论如何，以后OS课本中，context管理肯定是核心章节",
          "askedAt": "2025-10-24T19:03:23+08:00",
          "responseMinutes": 14.4,
          "responders": [
            "IQ75"
          ]
        },
        {
          "questioner": "薇冷 Violet",
          "question": "那既然系统提示跟普通的用户消息，在模型训练中，模型最终所理解的那种注意力是不一样的吧？那厂商为什么不推出新的接口，比如提供一个被禁止的一个字段，然后通过训练层面的注意力分配，能够让模型某个指标下，比如说 95%，确保让他不做我们不希望他做的…",
          "askedAt": "2025-10-24T19:45:43+08:00",
          "responseMinutes": 7.3,
          "responders": [
            "Lex"
          ]
        },
        {
          "questioner": "薇冷 Violet",
          "question": "假设有这样一个禁止字段，那模型训练过程中是不是可以让它针对这部分内容做一个，就是比如像那种惩罚机制特别高，会有什么样的效果吗？畅想一下",
          "askedAt": "2025-10-24T19:47:23+08:00",
          "responseMinutes": 5.6,
          "responders": [
            "Lex"
          ]
        }
      ],
      "avgResponseMinutes": 26.5,
      "bestResponseHours": [
        18,
        15,
        17
      ]
    }
  },
  "talker": "27587714869@chatroom"
}
